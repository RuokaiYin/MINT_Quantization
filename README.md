# MINT_Quantization

## TODO:
I will clean up the codes soon... 

## Notice:
I found the code to have some errors when using different PyTorch versions. I will solve the problem later. 
For now, please run the code using PyTorch with version 1.13.0. This version is tested to be working. Thanks.

## Citing
If you find MINT is useful for your research, please use the following bibtex to cite us,

```
@inproceedings{yin2024mint,
  title={MINT: Multiplier-less INTeger Quantization for Energy Efficient Spiking Neural Networks},
  author={Yin, Ruokai and Li, Yuhang and Moitra, Abhishek and Panda, Priyadarshini},
  booktitle={2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)},
  pages={830--835},
  year={2024},
  organization={IEEE}
}
```
